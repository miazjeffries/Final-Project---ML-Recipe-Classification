PROJECT TITLE: 
    ML Recipe Classification

PURPOSE OF PROJECT: 
    The purpose of this project is to build, train, and evaluate models that classify cooking recipes into broader categories (e.g. Main Dish, Dessert, Beverage, etc.) Using both a baseline Random Forest and a fine-tuned DistilBERT model, this project explores how traditional machine learning can compare to modernized, transformer-based NLP methods. This project includes data processing, category consolidation, vectorization, model training, and evaluation.

VERSION or DATE:
    December 10, 2025

DATA:
The dataset files too large to upload to directly to GitHub. 
To obtain:
1. Original dataset ('recipes.csv')
    - Download from Kaggle here: https://www.kaggle.com/code/karltonkxb/food-recipe-association-rules
    - place file in the 'data/' folder

2. Processed dataset ('processed_data.csv')
    - Run the preprocess.py script to generate it from 'recipes.csv'
    - 'processed_data.csv' will be saved in the 'data/' folder

USER INSTRUCTIONS:
    1. Clone the repository to your local machine
    2. Install required libraries using 'pip install -r requirements.txt'
    3. To process original data, run 'preprocess.py'
    4. To run Random Forest model, run 'baseline_rf.py'
    5. To run DistilBERT fine-tuning script, run 'distilbert.py'
    6. All metrics, predictions, and reports are saved to the 'results/' directory

AUTHORS:
    Mia Jeffries, 
    Talisa Pham-Quang, 
    Trisha Bhima

NOTES:
    - Ensure the processed dataset is formed and located in the 'data/' folder
    - Run the baseline model first if you would like comparison metrics
    - The 'results'/ directory contains all saved accuracy scores, classification reports, and prediction files
    - 'distilbert_recipe_model' contains the trained transformer model
    - Hyperparameters, additional models, or target categories can be modified as desired
    - Due to file size limitations, the fine-tuned DistilBERT model weights are not included in this repository.
    - The model can be regenerated by running train_distilbert.py on processed_data.csv.
